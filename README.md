# ğŸ¤– **Machine Learning (ML) Projects Repository**

---

## ğŸ“˜ **Machine Learning**



Machine Learning is a branch of Artificial Intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed.
This subject focuses on **data preprocessing, model building, evaluation, and understanding learning behavior such as underfitting and overfitting**.

---

## ğŸ“‚ **About This Repository**

This repository contains **Machine Learning experiments and implementations** developed as part of academic coursework and self-learning.
The projects emphasize **theoretical understanding**, **practical implementation**, and **model performance analysis** using real-world datasets.

---

## ğŸ¯ **Objectives**

* To understand core **Machine Learning concepts**
* To implement **regression models** using Python
* To analyze **bias, variance, underfitting, and overfitting**
* To evaluate model performance using standard metrics
* To gain hands-on experience with real datasets

---

## ğŸ—‚ï¸ **Repository Structure**

```text
Machine-Learning-Projects/
â”‚
â”œâ”€â”€ ML-Theory-1-House_price_Prediction (Regression + Underfitting)/
â”‚   â”œâ”€â”€ House-Price-Prediction.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ML-Theory-2-Bank_Marketing_Campaign/
|   â”œâ”€â”€ Bank_Marketing_Campaign.ipynb
|   â””â”€â”€ README.md
|
â”œâ”€â”€ ML-Lab-1-Implementation_of_Supervised_and_Unsupervised_Learning_Algorithms/
|   â”œâ”€â”€ Implementation-of-Supervised-and-Unsupervised-Learning-Algorithms.ipynb
|   â””â”€â”€ README.md
|
â”œâ”€â”€ ML-Theory-3-Bike-Demand-Forecasting-using-Subagging-vs-Bagging-vs-Boosting-(Regression)/
|   â”œâ”€â”€ Bike-Demand-Forecasting-using-Subagging-vs-Bagging-vs-Boosting-(Regression).ipynb
|   â””â”€â”€ README.md
|
â”œâ”€â”€ ML-Theory-4-Ensemble-Combination-for-SMS-Spam-Classification/
|   â”œâ”€â”€ Ensemble-Combination-for-SMS-Spam-Classification.ipynb
|   â””â”€â”€ README.md
|
â””â”€â”€ README.md   â† (This file)
```

---

## ğŸ§ª **Experiments Included**

### ğŸ¡ **(Theory) Experiment 01: House Price Prediction**

**Problem Type:** Regression
**Dataset:** California Housing Prices
**Target Variable:** Median House Value

This experiment focuses on:

* Predicting house prices using regression models
* Comparing training vs testing error
* Studying **underfitting (high bias)** and **overfitting (high variance)**

---

## ğŸ“š **Concepts Covered**

* Supervised Learning
* Regression Analysis
* Trainâ€“Test Split
* Feature Scaling
* Model Evaluation Metrics (RMSE, MAE)
* Bias vs Variance Trade-off
* Regularization (Ridge Regression)
* Ensemble Learning (Random Forest â€“ extension)

---


## ğŸ› ï¸ **Tools & Technologies**

* ğŸ **Python 3.x**
* â˜ï¸ **Google Colab**
* ğŸ“Š **Pandas & NumPy** â€“ Data handling
* ğŸ“ˆ **Scikit-Learn** â€“ ML models & metrics
* ğŸ“‰ **Matplotlib / Seaborn** â€“ Visualization
* ğŸŒ **Git & GitHub** â€“ Version control

---

## ğŸš€ **How to Use This Repository**

1. Clone the repository
2. Open the required experiment folder
3. Launch the notebook in **Google Colab**
4. Upload the dataset if prompted
5. Run cells sequentially to view results

---

## ğŸ“ **Learning Outcomes**

After completing these experiments, learners will be able to:

* Build and evaluate regression models
* Interpret model performance using error metrics
* Identify underfitting and overfitting scenarios
* Apply feature scaling and preprocessing techniques
* Understand real-world ML challenges

---

## ğŸ« **Academic Relevance**

* Aligned with **Machine Learning syllabus**
* Suitable for **lab experiments, assignments, and viva**
* Demonstrates practical application of theoretical concepts
* Supports **Bloomâ€™s Taxonomy (Apply, Analyze, Evaluate)**

---

## ğŸ”® **Future Scope & Extensions**

* Add **classification experiments**
* Hyperparameter tuning using GridSearchCV
* Cross-validation techniques
* Deep Learning models
* Deployment using Flask or Streamlit
* Additional Kaggle competitions

---

## ğŸ“Œ **References**

* Scikit-Learn Documentation
* Google Colab Tutorials
* Kaggle â€“ California Housing Dataset
* Machine Learning by Tom Mitchell

---

## ğŸ™ **Acknowledgement**

I would like to thank my **faculty**, **online learning platforms**, and **open-source communities** for providing valuable resources and guidance throughout this learning journey.

---

## ğŸ”— **Project Link**

ğŸ“˜ **Google Colab Notebook:**  [https://colab.research.google.com/drive/16xFA0CYw6FZ7GOGOaXteRmjfIn6IMYrA#scrollTo=JeSEDme1A8KR](https://colab.research.google.com/drive/16xFA0CYw6FZ7GOGOaXteRmjfIn6IMYrA#scrollTo=JeSEDme1A8KR)

ğŸ“Š **Kaggle** :  [https://www.kaggle.com/datasets/camnugent/california-housing-prices]


---


## ğŸ¦(Theory) Experiment 02: Bank Marketing Campaign â€“ Term Deposit Prediction

**Problem Type:** Classification  
**Dataset:** UCI Bank Marketing Dataset  
**Target Variable:** Term Deposit Subscription (`y` â€“ yes / no)

---

### This experiment focuses on:

- Predicting whether a bank customer will subscribe to a term deposit  
- Building a binary classification model using Logistic Regression  
- Evaluating model performance beyond accuracy  
- Understanding precisionâ€“recall trade-off and threshold optimization  

---

## ğŸ“š Concepts Covered

- Supervised Learning  
- Binary Classification  
- Logistic Regression  
- Trainâ€“Test Split  
- Confusion Matrix  
- Precision, Recall, and F1-score  
- Sensitivity and Specificity  
- ROC Curve and ROC-AUC  
- Threshold-based decision making  

---

## ğŸ› ï¸ Tools & Technologies

ğŸ Python 3.x  
â˜ï¸ Google Colab  

ğŸ“Š Pandas & NumPy â€“ Data handling  
ğŸ“ˆ Scikit-Learn â€“ ML models & metrics  
ğŸ“‰ Matplotlib / Seaborn â€“ Visualization  
ğŸŒ Git & GitHub â€“ Version control  

---

## ğŸš€ How to Use This Repository

- Clone the repository  
- Open the **Bank Marketing Campaign** experiment folder  
- Launch the notebook in **Google Colab**  
- Upload and extract the dataset (ZIP/CSV) if prompted  
- Run cells sequentially to view predictions and evaluation results  

---

## ğŸ“ Learning Outcomes

After completing this experiment, learners will be able to:

- Build and evaluate a Logistic Regression classification model  
- Interpret confusion matrix and classification metrics  
- Understand the importance of ROC curves  
- Analyze precisionâ€“recall trade-offs  
- Apply threshold tuning for business-oriented predictions  

---

## ğŸ« Academic Relevance

- Aligned with Machine Learning syllabus  
- Suitable for classification lab experiments, assignments, and viva  
- Demonstrates practical application of theoretical classification concepts  
- Supports Bloomâ€™s Taxonomy (Apply, Analyze, Evaluate)  

---

## ğŸ”® Future Scope & Extensions

- Try advanced classifiers (SVM, Decision Tree, Random Forest)  
- Hyperparameter tuning using GridSearchCV  
- Cross-validation techniques  
- Cost-sensitive learning  
- Model deployment using Flask or Streamlit  
- Participation in real-world marketing analytics challenges  

---

## ğŸ“Œ References

- Scikit-Learn Documentation  
- UCI Machine Learning Repository â€“ Bank Marketing Dataset  
- Google Colab Tutorials  
- Machine Learning by Tom Mitchell  

---

## ğŸ™ Acknowledgement

I would like to thank my faculty, online learning platforms, and open-source communities for their valuable guidance and resources in understanding classification models and evaluation techniques.

---

## ğŸ”— Project & Dataset Links
ğŸ“˜ **Google Colab Notebook** ğŸ‘‰ [https://colab.research.google.com/drive/1LeoTkdZObKmTU0Ll8T77f0_3oQsCg2-z?usp=sharing]

ğŸ“Š **UCI Bank Marketing Dataset** ğŸ‘‰ [https://archive.ics.uci.edu/dataset/222/bank+marketing]


---


## ğŸ  (Lab) Experiment 01: Supervised & Unsupervised Learning â€“ Housing Data Analysis

**Problem Type:** Regression & Clustering  
**Dataset:** USA Housing Dataset  
**Target Variable:** House Price (`Price`)

---

### This experiment focuses on:

- Predicting house prices using **Supervised Learning (Linear Regression)**  
- Discovering natural groupings in housing data using **Unsupervised Learning (K-Means Clustering)**  
- Comparing prediction-based and pattern-discovery-based learning paradigms  
- Understanding biasâ€“variance considerations and business implications  

---

## ğŸ“š Concepts Covered

- Supervised Learning  
- Unsupervised Learning  
- Linear Regression  
- K-Means Clustering  
- Trainâ€“Test Split  
- Feature Scaling  
- Regression Evaluation Metrics (MAE, MSE, RMSE)  
- Biasâ€“Variance Trade-off  
- Business Interpretation of ML Models  

---

## ğŸ› ï¸ Tools & Technologies

ğŸ Python 3.x  
â˜ï¸ Google Colab  

ğŸ“Š Pandas & NumPy â€“ Data preprocessing and handling  
ğŸ“ˆ Scikit-Learn â€“ ML models and evaluation  
ğŸ“‰ Matplotlib & Seaborn â€“ Visualization  
ğŸŒ Git & GitHub â€“ Version control and documentation  

---

## ğŸš€ How to Use This Repository

- Clone the repository  
- Open the **Supervised & Unsupervised Learning** experiment folder  
- Launch the notebook in **Google Colab**  
- Upload the `USA_Housing.csv` dataset if prompted  
- Run all cells sequentially to view predictions, clusters, and insights  

---

## ğŸ“Š Dataset Description

- **Dataset Name:** USA Housing Dataset  
- **Source:** Kaggle  
- **Records:** 5000  
- **Features:** 7  

The dataset consists of numerical housing attributes such as income, population, and house characteristics.  
The `Address` column is excluded from modeling.

---

## ğŸ¯ Target Variable

- **Price** â€“ Continuous variable representing house price (used in supervised learning).

---

## ğŸ“¥ Input Features

- Avg. Area Income  
- Avg. Area House Age  
- Avg. Area Number of Rooms  
- Avg. Area Number of Bedrooms  
- Area Population  

---

## ğŸ” Methodology

### ğŸ”¹ Supervised Learning (Linear Regression)
- Selected numerical features and target variable
- Split data into training and testing sets
- Trained Linear Regression model
- Evaluated performance using MAE, MSE, and RMSE
- Visualized Actual vs Predicted house prices

### ğŸ”¹ Unsupervised Learning (K-Means Clustering)
- Removed target variable and categorical features
- Applied feature scaling using StandardScaler
- Performed clustering with K-Means
- Visualized clusters based on income and population

---

## ğŸ“ˆ Results & Insights

- Linear Regression produced reliable house price predictions.
- Regression plot showed strong correlation between actual and predicted values.
- K-Means clustering revealed meaningful groupings in housing data.
- Supervised learning enabled quantitative evaluation.
- Unsupervised learning provided exploratory insights without labels.

---

## âš–ï¸ Biasâ€“Variance & Business Perspective

- Linear Regression offers **low variance** and **moderate bias**, making it stable and interpretable.
- K-Means is sensitive to feature scaling and choice of cluster count.
- From a business standpoint:
  - Supervised learning is ideal for **price prediction**
  - Unsupervised learning supports **market segmentation and planning**

---

## â–¶ï¸ Step-by-Step Execution

1. Open the Google Colab notebook  
2. Upload the `USA_Housing.csv` dataset  
3. Execute all cells in order  
4. Review evaluation metrics and visualizations  
5. Interpret supervised vs unsupervised outcomes  

---

## ğŸ“ Notes

- Feature scaling is essential for clustering algorithms.
- Linear Regression assumes linear relationships.
- Evaluation is objective in supervised learning and exploratory in unsupervised learning.

---

## ğŸ“ Viva-Voce Key Points

- Supervised learning uses labeled data; unsupervised learning does not.
- Linear Regression predicts continuous values.
- K-Means groups similar data points based on distance.
- Feature scaling improves clustering performance.
- Algorithm choice depends on problem type and data availability.

---

## ğŸ Conclusion

This experiment successfully demonstrated the application of **Supervised and Unsupervised Learning techniques** using a real-world housing dataset. Linear Regression effectively predicted house prices when labeled data was available, while K-Means clustering uncovered hidden structures within the dataset. The experiment highlights the importance of selecting appropriate machine learning approaches based on data characteristics, business goals, and evaluation requirements.

---

## ğŸ”— Project & Dataset Links

ğŸ“˜ **Google Colab Notebook**ğŸ‘‰ [https://colab.research.google.com/drive/1PQRBhoPnNgC-NJRkUefkyjA3-xRaAluA]

ğŸ“Š **Dataset Reference**ğŸ‘‰ [https://www.kaggle.com/code/fatmakursun/supervised-unsupervised-learning-examples/notebook]

---

## ğŸ™ Acknowledgement

I would like to thank my faculty, online learning platforms, and the open-source community for their guidance and resources in understanding supervised and unsupervised machine learning concepts.

---


## ğŸ  (Lab) Experiment 02: Implementation of Linear and Nonlinear Regression Models  


## ğŸ¯ PROJECT OBJECTIVE  
The objective of this project is to implement and compare **Linear Regression (Supervised Learning)** and **Nonlinear Regression (Polynomial Regression)** using a real-world medical insurance dataset.  

The experiment aims to analyze how different regression techniques perform in predicting insurance charges and to understand the importance of modeling nonlinear relationships in real-world data.

---

## ğŸ§© PROBLEM STATEMENT  
Medical insurance charges depend on several demographic and health-related factors such as age, BMI, smoking habits, and region.  

This experiment addresses:  

- Predicting medical insurance charges using Linear Regression  
- Improving prediction accuracy using Nonlinear (Polynomial) Regression  
- Comparing model performance using evaluation metrics  

The goal is to determine whether a simple linear model is sufficient or a nonlinear model better captures data patterns.

---

## ğŸ“Š DATASET DESCRIPTION  
**Dataset Name:** Medical Insurance Cost Dataset  
**Source:** Kaggle  
**Number of Records:** 1338  
**Number of Features:** 7 (6 input features + 1 target variable)  

The dataset contains both numerical and categorical attributes describing individuals and their corresponding insurance charges.

---

## ğŸ¯ TARGET VARIABLE  
**charges** â€“ Represents the medical insurance cost (continuous numerical value).

---

## ğŸ“¥ INPUT FEATURES  

- age â€“ Age of the individual  
- sex â€“ Gender  
- bmi â€“ Body Mass Index  
- children â€“ Number of children  
- smoker â€“ Smoking status  
- region â€“ Residential region  

(Categorical variables were converted into numerical form using encoding techniques.)

---

## ğŸ“‚ FOLDER CONTENTS  

|           File Name               |                     Description                      |
|-----------------------------------|------------------------------------------------------|
| Insurance_Regression.ipynb        | Colab notebook containing implementation and outputs |
| insurance.csv                     |         Dataset used for training and testing        |
| README.md                         |                 Project documentation                |

---

## ğŸ› ï¸ TOOLS & TECHNOLOGIES  

- ğŸ Python 3  
- â˜ï¸ Google Colab  
- ğŸ“Š Pandas  
- ğŸ”¢ NumPy  
- ğŸ“ˆ Matplotlib  
- ğŸ¤– Scikit-learn  
- ğŸ“ PolynomialFeatures  

---

## ğŸ” METHODOLOGY  

### Linear Regression  
- Selected input features and target variable  
- Encoded categorical variables  
- Performed trainâ€“test split (80:20)  
- Trained Linear Regression model  
- Evaluated using MAE, MSE, RMSE, and RÂ² Score  

### Nonlinear Regression (Polynomial Regression)  
- Applied Polynomial Feature transformation (degree 2)  
- Trained regression model on transformed features  
- Compared performance with Linear Regression  
- Visualized prediction performance  

---

## ğŸ“ˆ RESULTS & INSIGHTS  

- Linear Regression provided a strong baseline prediction model.  
- Polynomial Regression captured nonlinear relationships more effectively.  
- Nonlinear model showed improved RÂ² score and lower RMSE.  
- Smoking status and BMI significantly influenced insurance charges.  
- Real-world datasets often contain nonlinear patterns that simple linear models may not fully capture.  

---

## âš–ï¸ BIASâ€“VARIANCE & BUSINESS PERSPECTIVE  

**Linear Regression:**  
- Higher bias (may underfit complex relationships)  
- Low variance and highly interpretable  

**Polynomial Regression:**  
- Lower bias  
- Slightly higher variance (risk of overfitting if degree is high)  

**Business Perspective:**  
- Accurate prediction helps insurance companies in premium pricing  
- Assists in risk assessment and customer segmentation  
- Supports financial planning and decision-making  

---

## â–¶ï¸ STEP-BY-STEP EXECUTION  

1. Open the Google Colab notebook  
2. Upload the dataset (insurance.csv)  
3. Import required libraries  
4. Perform data preprocessing and encoding  
5. Define features (X) and target (y)  
6. Split dataset into training and testing sets  
7. Train Linear Regression model  
8. Apply Polynomial transformation and train Nonlinear model  
9. Evaluate models using performance metrics  
10. Compare results and draw conclusions  

---

## ğŸ“ NOTES  

- Ensure dataset contains no missing values before training  
- Avoid using high polynomial degree to prevent overfitting  
- Use multiple evaluation metrics for proper comparison  
- Feature encoding is necessary for categorical variables  

---

## ğŸ“ VIVA-VOCE KEY POINTS  

- Difference between Linear and Polynomial Regression  
- Meaning of RÂ² Score, MAE, MSE, RMSE  
- Concept of overfitting and underfitting  
- Importance of encoding categorical variables  
- Applications of regression in real-world industries  

---

## ğŸ CONCLUSION  

This experiment successfully demonstrated the implementation of both Linear and Nonlinear Regression techniques on the Medical Insurance dataset. While Linear Regression provided a simple and interpretable model, Polynomial Regression improved predictive accuracy by capturing nonlinear relationships. The experiment highlights the importance of selecting appropriate regression models based on data complexity and business objectives.

---

## ğŸ”— PROJECT & DATASET LINKS  

**Google Colab Notebook:** ğŸ‘‰  [https://colab.research.google.com/drive/13pIGt7cZv7mj8DQ3_PMWenSCQkOyllPG] 

**Dataset:** ğŸ‘‰ [https://www.kaggle.com/datasets/mirichoi0218/insurance] 

---

## ğŸ™Œ ACKNOWLEDGEMENT  

This project was carried out as part of the Machine Learning Laboratory to gain practical understanding of regression techniques using real-world healthcare cost data.

---

# ğŸ“˜ (Theory) Experiment 03: Bike Demand Forecasting using Ensemble Regression (Bagging, Subagging & Boosting)

---

## ğŸ“Œ Project Objective
The objective of this project is to forecast hourly bike rental demand (`cnt`) using ensemble regression models including Bagging (Random Forest), Subagging (BaggingRegressor with `max_samples < 1.0`), and Boosting (GradientBoostingRegressor). Prediction accuracy is evaluated using 5-Fold cross validation.

---

## ğŸ§© Problem Statement
Urban mobility teams need accurate bike demand predictions to improve fleet distribution and minimize shortages. This experiment uses the UCI Bike Sharing dataset to build and compare ensemble regression models and determine which approach generalizes best for forecasting hourly bike rentals.

---

## ğŸ“Š Dataset Description
Dataset used: **hour.csv** from the UCI Bike Sharing Dataset (UCI Machine Learning Repository).  
It contains hourly records of bike rental counts along with weather and seasonal attributes collected over two years from the Capital Bikeshare system in Washington, D.C., USA. :contentReference[oaicite:1]{index=1}

The dataset includes 17,389 instances with features such as hour of the day, season, temperature, humidity, windspeed, and target rental count (`cnt`). :contentReference[oaicite:2]{index=2}

---

## ğŸ¯ Target Variable
- **cnt** â€“ Total count of bike rentals (sum of casual + registered) per hour.

---

## ğŸ“¥ Input Features
The model uses the following input features after preprocessing:
- season â€“ Season of the year
- yr â€“ Year (0: 2011, 1: 2012)
- mnth â€“ Month of the year
- hr â€“ Hour of the day
- holiday â€“ Whether the day is a holiday
- weekday â€“ Day of the week
- workingday â€“ Whether itâ€™s a working day
- weathersit â€“ Weather situation
- temp â€“ Normalized temperature
- atemp â€“ Normalized feeling temperature
- hum â€“ Normalized humidity
- windspeed â€“ Normalized wind speed

*Note:* `instant`, `dteday`, `casual`, and `registered` features are removed before modeling.

---

## ğŸ“‚ Folder Contents

|                                     File Name                                       |                     Description                       |
|-------------------------------------------------------------------------------------|-------------------------------------------------------|
|                                    `hour.csv`                                       | UCI Bike Sharing dataset used for regression analysis |
| `Bike-Demand-Forecasting-using-Subagging-vs-Bagging-vs-Boosting-(Regression).ipynb' |     Colab notebook with implementation and outputs    |                              |                            `cv_regression_results.csv`                              |           Cross validation metrics per model          |
|                              `final_predictions.csv`                                |             Predicted vs actual bike counts           |
|                                    `README.md`                                      |                  Project documentation                | 

---

## ğŸ› ï¸ Tools & Technologies
- ğŸ Python 3  
- â˜ï¸ Google Colab  
- ğŸ“Š Pandas  
- ğŸ”¢ NumPy  
- ğŸ¤– Scikit-Learn  
- ğŸ“ˆ Matplotlib / Seaborn  

---

## ğŸ“‹ Methodology
1. Load and preprocess the dataset (drop unused columns, encode categorical variables).
2. Define 5-Fold Cross Validation.
3. Train and evaluate:
   - RandomForestRegressor (Bagging)
   - BaggingRegressor with DecisionTree (Subagging)
   - GradientBoostingRegressor (Boosting)
4. Use cross validation to calculate mean RMSE and MAE with standard deviations.
5. Train best model on full dataset.
6. Save predictions and feature importance.

---

## ğŸ“ˆ Results & Insights
- Cross-validation results include average RMSE and MAE for each model.
- Ensemble methods show better generalization compared to single tree models.
- Models with weaker bias and proper complexity show stronger performance.
- Feature importance shows which inputs most influence bike rental counts (e.g., hour, temperature, humidity).

---

## âš–ï¸ Bias & Variance Analysis
- **Bagging (Random Forest)**: Reduces variance by averaging many decorrelated trees.
- **Subagging**: Introduces additional randomness via subsampling to further reduce variance.
- **Boosting (Gradient)**: Reduces bias by sequentially learning from residuals.
- Optimal model selection depends on the balance between bias and variance for this dataset.

---

## ğŸ§  Step-by-Step Execution

1. Upload `hour.csv` to Colab.
2. Load data and inspect initial rows.
3. Drop irrelevant columns (`instant`, `casual`, `registered`, etc.).
4. Define features (X) and target (`cnt`).
5. Create 5-Fold cross validator.
6. Define ensemble models (Random Forest, Subagging, Boosting).
7. Perform cross validation to compute RMSE and MAE.
8. Save `cv_regression_results.csv`.
9. Train best model on full dataset and save `final_predictions.csv`.
10. Plot performance and feature importance.

---

## ğŸ“ Notes
- Ensure categorical variables are numeric before modeling.
- Scale/normalize features if needed for some algorithms.
- Keep cross validation consistent for fair comparison.
- Use error metrics with both mean and standard deviation.

---

## ğŸ“ Viva-Voce Key Points
- Difference between Bagging, Subagging, and Boosting.
- Meaning and interpretation of cross-validation metrics (RMSE, MAE).
- How ensembles reduce bias/variance.
- Feature importance and its significance.
- Real-world relevance of demand forecasting.

---

## ğŸ Conclusion
The ensemble regression models show that Boosting (or the best performing model in your results) provides the most accurate hourly bike demand forecasting based on cross validation metrics. Ensemble approaches reduce error reliably and improve generalization compared to basic regression techniques, thus making them suitable for real-world forecasting problems like bike rental demand.

---

## ğŸ”— Project & Dataset Links
**Google Colab Notebook:** ğŸ‘‰[https://colab.research.google.com/drive/1cCB5qMoc0lz-v-Sz6jnFmt2b9TSvTWIu?usp=sharing]

**Dataset:** ğŸ‘‰[https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset] 

---

## ğŸ™Œ Acknowledgement
This project was conducted as part of a machine learning practicum to understand and compare ensemble regression models using real-world bike sharing data. Thanks to the UCI Machine Learning Repository for providing open-access datasets for educational purposes. 

---

# ğŸ  (Theory) Experiment 4: Ensemble Learning for SMS Spam Classification Using Voting, Stacking, and AdaBoost

---

# ğŸ¯ Project Objective

The objective of this project is to implement and compare multiple classifier combination techniques for SMS spam detection. The project evaluates individual base learners and ensemble strategies including Hard Voting, Soft Voting, Stacking, and AdaBoost with decision stumps using Stratified K-Fold Cross Validation.

---

# ğŸ“„ Problem Statement  

A messaging platform wants to automatically classify SMS messages as **Spam** or **Ham (Not Spam)**.

The goal is to:

- Train individual machine learning classifiers  
- Combine classifiers using ensemble techniques  
- Implement AdaBoost using decision stumps (max_depth = 1)  
- Compare performance using Precision, Recall, F1-score, ROC-AUC  
- Recommend the best combining strategy  

---

# ğŸ“Š Dataset Description  

Dataset used: SMS Spam Collection (UCI Dataset ID: 228)

This dataset contains 5,574 SMS messages labeled as spam or ham.

Each record consists of:
- Label (ham or spam)  
- Message text  

---

# ğŸ¯ Target Variable  

**label**

- 0 â†’ Ham  
- 1 â†’ Spam  

---

# ğŸ“ Folder Contents  

```
Spam-Ensemble-Project
 â”£ task4_spam_ensemble_combination.py
 â”£ sms.csv
 â”£ ensemble_comparison.csv
 â”£ final_model_predictions.csv
 â”£ README.md
```

---

# ğŸ“¥ Input Features  

| Feature  |   Description    |
|----------|------------------|
| message  | SMS text content |

Text is converted into numerical format using **TF-IDF vectorization**.

---

# ğŸ›  Tools & Technologies  

- Python  
- Scikit-learn  
- Pandas  
- NumPy  
- TF-IDF Vectorizer  
- Google Colab  

---

# âš™ Methodology  

## 1ï¸âƒ£ Data Preprocessing  
- Convert dataset to CSV format  
- Label Encoding (ham=0, spam=1)  
- TF-IDF Vectorization  

## 2ï¸âƒ£ Base Learners  
- Multinomial Naive Bayes  
- Logistic Regression  
- Linear SVM  

## 3ï¸âƒ£ Ensemble Methods  
- Hard Voting  
- Soft Voting  
- Stacking (Meta-Learner: Logistic Regression)  
- AdaBoost with Decision Stumps (max_depth=1)  

## 4ï¸âƒ£ Evaluation Strategy  
- Stratified 5-Fold Cross Validation  
- Metrics:
  - Precision  
  - Recall  
  - F1-score  
  - ROC-AUC  
- Confusion Matrix  

---

# ğŸ“ˆ Results & Insights  

Key Observations:

- Logistic Regression and Linear SVM performed strongly as individual models.
- Hard Voting improved stability but ignored probability information.
- Soft Voting performed better than Hard Voting.
- Stacking provided better generalization by learning optimal combination weights.
- AdaBoost with stumps improved weak learners but was slightly less effective on high-dimensional TF-IDF features.

---

### ğŸ”¥ Best Performing Model:
**Stacking Classifier**

It achieved the highest F1-score and ROC-AUC with stable cross-validation performance.

---

# âš– Bias & Variance Analysis  

|       Model         |       Bias       |            Variance              |
|---------------------|------------------|----------------------------------|
| Naive Bayes         | High Bias        |           Low Variance           |
| Logistic Regression | Moderate         |            Moderate              |
| Linear SVM          | Low Bias         |      Slightly Higher Variance    |
| Hard Voting         | Reduced Variance |          Moderate Bias           |
| Stacking            | Balanced         |         Reduced Variance         |
| AdaBoost            | Low Bias         | Can increase variance if overfit |

Stacking achieves the best bias-variance tradeoff.

---

# â–¶ Step-by-Step Execution  

## Step 1  
Install required libraries (if using Colab)

## Step 2  
Run the script

## Step 3  
Check generated files

---

# ğŸ“ Notes  

- Stratified K-Fold ensures balanced spam/ham distribution.  
- AdaBoost uses decision stumps (max_depth=1) as required.  
- TF-IDF improves text feature representation.  
- Stacking uses Logistic Regression as meta-learner.  

---

# ğŸ“ Viva-Voce Key Points  

1. Why use TF-IDF instead of CountVectorizer?  
   â†’ TF-IDF reduces importance of common words.

2. Difference between Hard and Soft Voting?  
   â†’ Hard uses majority voting; Soft uses probability averaging.

3. What is a Decision Stump?  
   â†’ A decision tree with depth = 1.

4. Why Stratified K-Fold?  
   â†’ Maintains class balance in each fold.

5. Why Stacking performed best?  
   â†’ Meta-learner learns optimal combination of base models.

6. How does AdaBoost work?  
   â†’ Sequentially focuses on misclassified samples.

---

# ğŸ Conclusion  

This project demonstrates that ensemble methods significantly improve spam classification performance.

Among all combining strategies, **Stacking Classifier** achieved the best balance between bias and variance and delivered the highest F1-score and ROC-AUC.

Therefore, stacking is recommended for production-level SMS spam filtering systems.

---

# ğŸ”— Project & Dataset Links  

**Google Colab Notebook**: ğŸ‘‰[https://colab.research.google.com/drive/14Bf7eai2Bk16McOK-TKvkXsUv23d9PU5#scrollTo=OOGw3mUb1oHr] 

**Dataset Link**:ğŸ‘‰[https://archive.ics.uci.edu/dataset/228/sms+spam+collection] 

---

# ğŸ™ Acknowledgement  

We thank the UCI Machine Learning Repository for providing the SMS Spam Collection dataset for academic use.
We also acknowledge the developers of Scikit-learn for providing powerful machine learning tools.

---
