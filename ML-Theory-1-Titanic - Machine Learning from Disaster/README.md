# ğŸš¢ Experiment 1: Titanic â€“ Machine Learning from Disaster

## ğŸ¯ Project Objective
The objective of this experiment is to apply **Machine Learning techniques** to analyze real-world structured data and understand how different models behave in terms of **bias and variance**.  
This experiment focuses on data preprocessing, feature scaling, training multiple regression models, and evaluating their performance.

---

## â“ Problem Statement
Predictive modeling on real-world datasets (such as Titanic-like disaster data) involves multiple interacting features.  
Simple models may underfit the data, while complex models may overfit.

**Problem:**  
Build and evaluate multiple Machine Learning models to:
- Learn relationships between features and target values  
- Analyze biasâ€“variance trade-off  
- Improve prediction accuracy using ensemble learning  

---

## ğŸ“Š Dataset Description
The dataset is loaded from an Excel file using Pandas.

### ğŸ¯ Target Variable
- `median_house_value`

### ğŸ“Œ Input Features
- longitude, latitude  
- housing_median_age  
- total_rooms, total_bedrooms  
- population, households  
- median_income  
- One-hot encoded categorical features  

---

## ğŸ“ Folder Contents
Titanic-Machine-Learning-from-Disaster/

â”œâ”€â”€ Titanic - Machine Learning from Disaster.ipynb

â”œâ”€â”€ housing.xlsx

â”œâ”€â”€ README.md

---

## ğŸ›  Tools & Technologies
- ğŸ§‘â€ğŸ’» Platform: Google Colab  
- ğŸ Language: Python  
- ğŸ“¦ Libraries:
  - pandas
  - numpy
  - scikit-learn
  - openpyxl
- ğŸ¤– Models Used:
  - Linear Regression
  - Ridge Regression
  - Decision Tree Regressor
  - Random Forest Regressor

---

## ğŸ” Methodology

### 1ï¸âƒ£ Data Loading
- Excel dataset loaded using `pandas.read_excel()`
- Data preview and column verification
- Missing values checked using `isnull().sum()`

### 2ï¸âƒ£ Feature Selection
- Target variable:
  - `median_house_value`
- Input features:
  - All remaining columns

### 3ï¸âƒ£ Trainâ€“Test Split
- 80% training data  
- 20% testing data  
- Ensures unbiased model evaluation

### 4ï¸âƒ£ Feature Scaling
- `StandardScaler` applied to numerical features
- Used for Linear & Ridge Regression
- Tree-based models trained without scaling

### 5ï¸âƒ£ Model Training
The following models were trained:
- Linear Regression
- Ridge Regression (regularization)
- Decision Tree Regression
- Random Forest Regression (ensemble learning)

### 6ï¸âƒ£ Model Evaluation
Models evaluated using:
- ğŸ“‰ Root Mean Squared Error (RMSE)
- ğŸ“ Mean Absolute Error (MAE)

---

## ğŸ“ˆ Results & Insights

### ğŸ” Model Performance Summary

|       Model       | Train RMSE | Test RMSE | Test MAE |
|-------------------|------------|-----------|----------|
| Linear Regression |   High     |   High    |   High   |
| Ridge Regression  |   High     |   High    |   High   |
| Decision Tree     |    0.0     |   High    |   High   |
| Random Forest     |   18118    |   49038   |  31639   |

### ğŸ§  Biasâ€“Variance Analysis
- **Linear & Ridge Regression**  
  ğŸ”¹ High bias â†’ underfitting  
  ğŸ”¹ Fail to capture non-linear relationships  

- **Decision Tree**  
  ğŸ”¸ High variance â†’ overfitting  
  ğŸ”¸ Perfect training accuracy but poor test results  

- **Random Forest**  
  âœ… Reduces overfitting using multiple trees  
  âœ… Better generalization and stability  

---

## ğŸ§ª Step-by-Step Execution
1. Install dependency:
   ```bash
   pip install openpyxl
2. Load dataset using Pandas

3. Check missing values and columns

4. Split dataset into training & testing sets

5. Apply feature scaling

6. Train models

7. Evaluate using RMSE and MAE

8. Compare results and analyze performance

---

## ğŸ“ Notes

1. Feature scaling is essential for linear models

2. Decision Trees do not require scaling

3. Ensemble models improve robustness

4. Model selection depends on biasâ€“variance trade-off

---

## âœ… Conclusion

This experiment demonstrates a complete Machine Learning workflow including data preprocessing, model training, evaluation, and biasâ€“variance analysis.
The use of ensemble learning improves model generalization and highlights best practices for real-world predictive modeling.

---

## ğŸ“š References
Google Colab Documentation: [https://colab.research.google.com/]

Kaggle Titanic Dataset: [https://www.kaggle.com/datasets/c/titanic]
